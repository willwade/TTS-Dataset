Metadata-Version: 2.4
Name: tts-dataset
Version: 0.1.0
Summary: Monthly TTS voice catalog dataset pipeline using py3-tts-wrapper
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: sqlite-utils
Requires-Dist: langcodes
Requires-Dist: py3-tts-wrapper[google,googletrans,microsoft,openai,playht,polly,sherpaonnx,watson,witai]>=0.10.23
Requires-Dist: py3-tts-wrapper[sapi,uwp]>=0.10.23; sys_platform == "win32"
Requires-Dist: py3-tts-wrapper[avsynth]>=0.10.23; sys_platform == "darwin"
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"

# TTS Voice Catalog Dataset

A monthly-updated dataset of Text-to-Speech voices collected from multiple sources using `py3-tts-wrapper`. Data is published to Datasette for browsable exploration and API access.

## Overview

This project automatically collects platform TTS voice information:

| Category | Sources | Voices Source |
|----------|---------|---------------|
| **Platform Engines** | Windows SAPI5 (+ UWP best-effort), macOS AVSynth + eSpeak, Linux eSpeak | Native/system TTS APIs |

## Data Pipeline

```
GitHub Actions (Monthly)
        ↓
py3-tts-wrapper get_voices()
        ↓
data/raw/{platform}-voices.json
        ↓
scripts/harmonize.py (merge + enrich)
        ↓
data/voices.db (SQLite with FTS)
        ↓
Datasette on Vercel
```

## Quick Start

### Prerequisites

- Python 3.10+
- [UV](https://github.com/astral-sh/uv) package manager

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/TTS-Dataset.git
cd TTS-Dataset

# Install dependencies with UV
uv sync
```

### Local Usage

```bash
# Auto-detect platform and collect voices
uv run python scripts/collect_voices.py

# Collect online/credentialed engines (plus Sherpa-ONNX) and write per-engine JSON files
uv run python scripts/collect_voices.py --online

# List available local engines
uv run python scripts/collect_voices.py --list

# List all client engines exposed by py3-tts-wrapper
uv run python scripts/collect_voices.py --list-all

# Merge and enrich data, build SQLite database
uv run python scripts/harmonize.py

# Serve locally with Datasette
pip install datasette
datasette serve data/voices.db
```

## Environment Variables

Create `.env` from `.env.example` and configure:

| Variable | Required | Description |
|----------|-------------|
| `VERCEL_TOKEN` | Token for deploying to Vercel-hosted Datasette |

### Optional (for online engine collection)
| Variable | Service |
|----------|----------|
| `GOOGLE_TTS_CREDENTIALS` | Google Cloud TTS (path to JSON file) |
| `AZURE_TTS_KEY`, `AZURE_TTS_REGION` | Microsoft Azure TTS |
| `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION` | AWS Polly |
| `ELEVENLABS_API_KEY` | ElevenLabs |
| `WATSON_API_KEY`, `WATSON_REGION` | IBM Watson |
| `WITAI_TOKEN` | Wit.AI |

Note: the GitHub Actions workflow collects platform engines on all OSes and additionally runs `--online` on Linux when credentials are configured.

### Legacy (optional)
| Variable | Description |
|----------|-------------|
| `GOOGLE_GEOCODE_KEY` | Google Maps geocoding (legacy) |

## Data Schema

### Raw JSON Format

```json
{
  "id": "voice-identifier",
  "name": "Voice Display Name",
  "language_codes": ["en-US", "eng"],
  "gender": "Male|Female|Unknown",
  "platform": "windows|macos|linux",
  "collected_at": "2025-02-14T12:00:00Z"
}
```

### SQLite Schema (Datasette)

| Column | Type | Description |
|---------|------|-------------|
| `id` | TEXT | Primary key (voice identifier) |
| `name` | TEXT | Voice display name |
| `language_codes` | TEXT | JSON array of locale/ISO codes |
| `gender` | TEXT | Voice gender (if available) |
| `engine` | TEXT | TTS engine name |
| `platform` | TEXT | Source platform |
| `collected_at` | TEXT | ISO 8601 timestamp |
| `language_name` | TEXT | Enriched: English language name |
| `language_display` | TEXT | Enriched: Display language name |
| `country_code` | TEXT | Enriched: ISO 3166-1 alpha-2 |
| `script` | TEXT | Enriched: ISO 15924 script code |

## Project Structure

```
TTS-Dataset/
├── .github/
│   └── workflows/
│       └── update-voices.yml    # Monthly automation
├── data/
│   ├── raw/                        # JSON outputs (git-tracked)
│   └── voices.db                  # Build artifact (not in git)
├── scripts/
│   ├── collect_voices.py            # Simplified voice collection
│   └── harmonize.py                # Database build
├── tests/                           # Unit tests
├── pyproject.toml                   # UV config
├── .env.example                      # Environment template
├── .gitignore                       # Exclude .env, *.db
└── README.md
```

## GitHub Actions

The workflow runs monthly on first day of each month:

1. **Collect**: Matrix build on Windows/macOS/Linux runners
2. **Harmonize**: Merge JSON, enrich with langcodes metadata
3. **Build**: Create SQLite database with full-text search
4. **Deploy**: Publish to Datasette on Vercel

Manual trigger available via Actions tab.
