name: Update TTS Voices Monthly

on:
  schedule:
    - cron: '0 0 1 * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  collect:
    name: Collect voices on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [windows-latest, macos-latest, ubuntu-latest]

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true

      - name: Install project
        run: uv sync

      - name: Reset raw output directory
        shell: pwsh
        run: |
          New-Item -ItemType Directory -Force data/raw | Out-Null
          Get-ChildItem data/raw -Filter *.json -File -ErrorAction SilentlyContinue | Remove-Item -Force

      - name: Install system dependencies (Linux)
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y portaudio19-dev espeak-ng

      - name: Install system dependencies (macOS)
        if: runner.os == 'macOS'
        run: brew install portaudio espeak-ng

      - name: Make AVSynth SpeechBridge executable (macOS)
        if: runner.os == 'macOS'
        shell: bash
        run: |
          set -euo pipefail
          bridge="$(find .venv -type f -path "*/tts_wrapper/engines/avsynth/SpeechBridge" | head -n 1 || true)"
          if [[ -n "${bridge}" ]]; then
            chmod +x "${bridge}"
            echo "Marked executable: ${bridge}"
          else
            echo "AVSynth SpeechBridge not found; continuing."
          fi

      - name: Install system dependencies (Windows)
        if: runner.os == 'Windows'
        run: echo "No system dependencies needed for Windows"

      - name: Collect voices
        run: uv run python scripts/collect_voices.py

      - name: Collect online voices (Linux)
        if: runner.os == 'Linux'
        env:
          GOOGLE_TTS_CREDENTIALS: ${{ secrets.GOOGLE_TTS_CREDENTIALS || vars.GOOGLE_TTS_CREDENTIALS }}
          GOOGLE_TTS_CREDENTIALS_JSON: ${{ secrets.GOOGLE_TTS_CREDENTIALS_JSON || vars.GOOGLE_TTS_CREDENTIALS_JSON }}
          GOOGLE_SA_FILE_B64: ${{ secrets.GOOGLE_SA_FILE_B64 || vars.GOOGLE_SA_FILE_B64 }}
          AZURE_TTS_KEY: ${{ secrets.AZURE_TTS_KEY || vars.AZURE_TTS_KEY }}
          AZURE_TTS_REGION: ${{ secrets.AZURE_TTS_REGION || vars.AZURE_TTS_REGION }}
          AWS_REGION: ${{ secrets.AWS_REGION || vars.AWS_REGION }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID || vars.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY || vars.AWS_SECRET_ACCESS_KEY }}
          ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY || vars.ELEVENLABS_API_KEY }}
          WATSON_API_KEY: ${{ secrets.WATSON_API_KEY || vars.WATSON_API_KEY }}
          WATSON_REGION: ${{ secrets.WATSON_REGION || vars.WATSON_REGION }}
          WATSON_INSTANCE_ID: ${{ secrets.WATSON_INSTANCE_ID || vars.WATSON_INSTANCE_ID }}
          WITAI_TOKEN: ${{ secrets.WITAI_TOKEN || vars.WITAI_TOKEN }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || vars.OPENAI_API_KEY }}
          PLAYHT_API_KEY: ${{ secrets.PLAYHT_API_KEY || vars.PLAYHT_API_KEY }}
          PLAYHT_USER_ID: ${{ secrets.PLAYHT_USER_ID || vars.PLAYHT_USER_ID }}
          UPLIFTAI_KEY: ${{ secrets.UPLIFTAI_KEY || vars.UPLIFTAI_KEY }}
        run: uv run python scripts/collect_voices.py --online

      - name: Upload raw data artifact
        uses: actions/upload-artifact@v4
        with:
          name: voices-${{ matrix.os }}
          path: data/raw/*.json
          retention-days: 30

  harmonize:
    name: Build database
    needs: collect
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true

      - name: Install project
        run: uv sync --extra dev

      - name: Download all voice artifacts
        uses: actions/download-artifact@v4
        with:
          path: data/raw
          pattern: voices-*
          merge-multiple: true

      - name: List downloaded files
        run: find data/raw -maxdepth 3 -type f -print

      - name: Install Playwright browser (best effort)
        continue-on-error: true
        run: uv run playwright install chromium

      - name: Refresh preview reference data (best effort)
        continue-on-error: true
        run: uv run python scripts/fetch_voice_previews.py

      - name: Import WorldAlphabets audio preview index (best effort)
        continue-on-error: true
        run: |
          git clone --depth 1 https://github.com/willwade/WorldAlphabets.git ../WorldAlphabets
          uv run python scripts/import_worldalphabets_audio.py --source ../WorldAlphabets/data

      - name: Build SQLite database
        run: uv run python scripts/harmonize.py

      - name: Export site data JSON
        run: uv run python scripts/export_site_data.py

      - name: Upload database artifact
        uses: actions/upload-artifact@v4
        with:
          name: voices-db
          path: data/voices.db
          retention-days: 90

      - name: Database statistics
        run: |
          echo "### Database Statistics" >> $GITHUB_STEP_SUMMARY
          uv run scripts/db_stats.py | tee -a $GITHUB_STEP_SUMMARY

      - name: Commit updated dataset files
        if: github.ref_type == 'branch'
        shell: bash
        run: |
          set -euo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          git add data/raw data/reference

          if git diff --cached --quiet; then
            echo "No data/raw or data/reference changes to commit."
            exit 0
          fi

          commit_msg="chore(data): update voice datasets (${GITHUB_RUN_ID})"
          git commit -m "${commit_msg}"
          git push origin "HEAD:${GITHUB_REF_NAME}"

  deploy:
    name: Deploy to Datasette
    needs: [harmonize]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment:
      name: datasette-production
      url: ${{ steps.deploy.outputs.url }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Vercel CLI
        run: npm install --global vercel@latest

      - name: Install deployment dependencies
        run: |
          pip install datasette datasette-publish-vercel datasette-vega

      - name: Download database
        uses: actions/download-artifact@v4
        with:
          name: voices-db
          path: data

      - name: Deploy to Vercel
        id: deploy
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          build_root="$(mktemp -d)"
          build_dir="${build_root}/datasette-app"

          # Generate Datasette deployment files without calling Vercel.
          datasette publish vercel data/voices.db \
            --token $VERCEL_TOKEN \
            --project tts-voice-catalog \
            --install datasette-vega \
            --title "Global TTS Voice Catalog" \
            --license "CC0-1.0" \
            --license_url "https://creativecommons.org/publicdomain/zero/1.0/" \
            --generate-dir "$build_dir"

          # Modern Vercel Python runtime expects handlers under /api.
          mkdir -p "$build_dir/api"
          mv "$build_dir/index.py" "$build_dir/api/index.py"
          cp .github/vercel.datasette.json "$build_dir/vercel.json"

          # Ensure we deploy to the intended Vercel project.
          vercel link --yes --project tts-voice-catalog --token="$VERCEL_TOKEN" --cwd "$build_dir"

          # For `vercel deploy`, stdout is the deployment URL.
          deploy_stdout="$(vercel deploy --prod --yes --token="$VERCEL_TOKEN" --cwd "$build_dir")"
          echo "$deploy_stdout"

          deploy_url="$(echo "$deploy_stdout" | grep -Eo 'https://[a-zA-Z0-9.-]+\.vercel\.app' | tail -n 1 || true)"
          if [[ -n "${deploy_url}" ]]; then
            echo "url=${deploy_url}" >> "$GITHUB_OUTPUT"
          fi

      - name: Create deployment summary
        run: |
          echo "### Datasette Deployment" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -n "${{ steps.deploy.outputs.url }}" ]; then
            echo "Deployment URL: ${{ steps.deploy.outputs.url }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "Deployment URL: (not captured, check deploy step logs)" >> $GITHUB_STEP_SUMMARY
          fi
